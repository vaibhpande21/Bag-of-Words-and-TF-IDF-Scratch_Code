{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755b2eeb",
   "metadata": {},
   "source": [
    "## Bag of Words & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75baff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I love natural language processing and deep learning\",\n",
    "    \"Word embeddings capture semantic relationships between words\",\n",
    "    \"The fox is quick and the dog is lazy\",\n",
    "    \"Deep learning models learn representations automatically\",\n",
    "    \"I love to learn about NLP and machine learning\",\n",
    "    \"The king and the queen shared their kingdom fairly\",\n",
    "    \"A man and a woman walked into the room\",\n",
    "    \"The queen loves her people and the king protects them\",\n",
    "    \"Natural language tasks include translation and summarization\"\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689aedd5",
   "metadata": {},
   "source": [
    "#### BOW Step by step implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2144f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for sentence in corpus:\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in vocab_list:\n",
    "            vocab_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a039405",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "107dcc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'and',\n",
       " 'automatically',\n",
       " 'between',\n",
       " 'brown',\n",
       " 'capture',\n",
       " 'deep',\n",
       " 'dog',\n",
       " 'embeddings',\n",
       " 'fairly',\n",
       " 'fox',\n",
       " 'her',\n",
       " 'i',\n",
       " 'include',\n",
       " 'into',\n",
       " 'is',\n",
       " 'jumps',\n",
       " 'king',\n",
       " 'kingdom',\n",
       " 'language',\n",
       " 'lazy',\n",
       " 'learn',\n",
       " 'learning',\n",
       " 'love',\n",
       " 'loves',\n",
       " 'machine',\n",
       " 'man',\n",
       " 'models',\n",
       " 'natural',\n",
       " 'nlp',\n",
       " 'over',\n",
       " 'people',\n",
       " 'processing',\n",
       " 'protects',\n",
       " 'queen',\n",
       " 'quick',\n",
       " 'relationships',\n",
       " 'representations',\n",
       " 'room',\n",
       " 'semantic',\n",
       " 'shared',\n",
       " 'summarization',\n",
       " 'tasks',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'to',\n",
       " 'translation',\n",
       " 'walked',\n",
       " 'woman',\n",
       " 'word',\n",
       " 'words'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08810783",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "for idx, word in enumerate(sorted(vocabulary)):\n",
    "    word_index[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f392e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'about': 1,\n",
       " 'and': 2,\n",
       " 'automatically': 3,\n",
       " 'between': 4,\n",
       " 'brown': 5,\n",
       " 'capture': 6,\n",
       " 'deep': 7,\n",
       " 'dog': 8,\n",
       " 'embeddings': 9,\n",
       " 'fairly': 10,\n",
       " 'fox': 11,\n",
       " 'her': 12,\n",
       " 'i': 13,\n",
       " 'include': 14,\n",
       " 'into': 15,\n",
       " 'is': 16,\n",
       " 'jumps': 17,\n",
       " 'king': 18,\n",
       " 'kingdom': 19,\n",
       " 'language': 20,\n",
       " 'lazy': 21,\n",
       " 'learn': 22,\n",
       " 'learning': 23,\n",
       " 'love': 24,\n",
       " 'loves': 25,\n",
       " 'machine': 26,\n",
       " 'man': 27,\n",
       " 'models': 28,\n",
       " 'natural': 29,\n",
       " 'nlp': 30,\n",
       " 'over': 31,\n",
       " 'people': 32,\n",
       " 'processing': 33,\n",
       " 'protects': 34,\n",
       " 'queen': 35,\n",
       " 'quick': 36,\n",
       " 'relationships': 37,\n",
       " 'representations': 38,\n",
       " 'room': 39,\n",
       " 'semantic': 40,\n",
       " 'shared': 41,\n",
       " 'summarization': 42,\n",
       " 'tasks': 43,\n",
       " 'the': 44,\n",
       " 'their': 45,\n",
       " 'them': 46,\n",
       " 'to': 47,\n",
       " 'translation': 48,\n",
       " 'walked': 49,\n",
       " 'woman': 50,\n",
       " 'word': 51,\n",
       " 'words': 52}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4de2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = np.zeros((len(corpus), len(vocabulary)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e70621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the matrix\n",
    "for i, sentence in enumerate(corpus):\n",
    "    words = sentence.lower().split()\n",
    "    for word in words:\n",
    "        if word in word_index:  # safety check\n",
    "            bow_matrix[i, word_index[word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a758078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>about</th>\n",
       "      <th>and</th>\n",
       "      <th>automatically</th>\n",
       "      <th>between</th>\n",
       "      <th>brown</th>\n",
       "      <th>capture</th>\n",
       "      <th>deep</th>\n",
       "      <th>dog</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>tasks</th>\n",
       "      <th>the</th>\n",
       "      <th>their</th>\n",
       "      <th>them</th>\n",
       "      <th>to</th>\n",
       "      <th>translation</th>\n",
       "      <th>walked</th>\n",
       "      <th>woman</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  about  and  automatically  between  brown  capture  deep  dog  \\\n",
       "0  0      0    0              0        0      1        0     0    1   \n",
       "1  0      0    1              0        0      0        0     1    0   \n",
       "2  0      0    0              0        1      0        1     0    0   \n",
       "3  0      0    1              0        0      0        0     0    1   \n",
       "4  0      0    0              1        0      0        0     1    0   \n",
       "5  0      1    1              0        0      0        0     0    0   \n",
       "6  0      0    1              0        0      0        0     0    0   \n",
       "7  2      0    1              0        0      0        0     0    0   \n",
       "8  0      0    1              0        0      0        0     0    0   \n",
       "9  0      0    1              0        0      0        0     0    0   \n",
       "\n",
       "   embeddings  ...  tasks  the  their  them  to  translation  walked  woman  \\\n",
       "0           0  ...      0    2      0     0   0            0       0      0   \n",
       "1           0  ...      0    0      0     0   0            0       0      0   \n",
       "2           1  ...      0    0      0     0   0            0       0      0   \n",
       "3           0  ...      0    2      0     0   0            0       0      0   \n",
       "4           0  ...      0    0      0     0   0            0       0      0   \n",
       "5           0  ...      0    0      0     0   1            0       0      0   \n",
       "6           0  ...      0    2      1     0   0            0       0      0   \n",
       "7           0  ...      0    1      0     0   0            0       1      1   \n",
       "8           0  ...      0    2      0     1   0            0       0      0   \n",
       "9           0  ...      1    0      0     0   0            1       0      0   \n",
       "\n",
       "   word  words  \n",
       "0     0      0  \n",
       "1     0      0  \n",
       "2     1      1  \n",
       "3     0      0  \n",
       "4     0      0  \n",
       "5     0      0  \n",
       "6     0      0  \n",
       "7     0      0  \n",
       "8     0      0  \n",
       "9     0      0  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(bow_matrix, columns=sorted(vocabulary))\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617e571",
   "metadata": {},
   "source": [
    "#### Count BoW Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23cf144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_representation(corpus, frequency = True):\n",
    "    vocabulary = set([x for x in \" \".join(corpus).lower().split(\" \")])\n",
    "\n",
    "    bow_rep = []\n",
    "    for sentence in corpus:\n",
    "        sentence_rep = dict([(v,0) for v in vocabulary])\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if frequency:\n",
    "                sentence_rep[word] += 1\n",
    "            else:\n",
    "                sentence_rep[word] = 1\n",
    "        bow_rep.append(sentence_rep)\n",
    "    return bow_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b25d9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>them</th>\n",
       "      <th>protects</th>\n",
       "      <th>fairly</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>shared</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>relationships</th>\n",
       "      <th>semantic</th>\n",
       "      <th>quick</th>\n",
       "      <th>...</th>\n",
       "      <th>into</th>\n",
       "      <th>fox</th>\n",
       "      <th>translation</th>\n",
       "      <th>language</th>\n",
       "      <th>to</th>\n",
       "      <th>people</th>\n",
       "      <th>woman</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>dog</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The quick brown fox jumps over the lazy dog</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love natural language processing and deep learning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word embeddings capture semantic relationships between words</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The fox is quick and the dog is lazy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep learning models learn representations automatically</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    them  protects  fairly  \\\n",
       "The quick brown fox jumps over the lazy dog            0         0       0   \n",
       "I love natural language processing and deep lea...     0         0       0   \n",
       "Word embeddings capture semantic relationships ...     0         0       0   \n",
       "The fox is quick and the dog is lazy                   0         0       0   \n",
       "Deep learning models learn representations auto...     0         0       0   \n",
       "\n",
       "                                                    embeddings  shared  is  \\\n",
       "The quick brown fox jumps over the lazy dog                  0       0   0   \n",
       "I love natural language processing and deep lea...           0       0   0   \n",
       "Word embeddings capture semantic relationships ...           1       0   0   \n",
       "The fox is quick and the dog is lazy                         0       0   2   \n",
       "Deep learning models learn representations auto...           0       0   0   \n",
       "\n",
       "                                                    love  relationships  \\\n",
       "The quick brown fox jumps over the lazy dog            0              0   \n",
       "I love natural language processing and deep lea...     1              0   \n",
       "Word embeddings capture semantic relationships ...     0              1   \n",
       "The fox is quick and the dog is lazy                   0              0   \n",
       "Deep learning models learn representations auto...     0              0   \n",
       "\n",
       "                                                    semantic  quick  ...  \\\n",
       "The quick brown fox jumps over the lazy dog                0      1  ...   \n",
       "I love natural language processing and deep lea...         0      0  ...   \n",
       "Word embeddings capture semantic relationships ...         1      0  ...   \n",
       "The fox is quick and the dog is lazy                       0      1  ...   \n",
       "Deep learning models learn representations auto...         0      0  ...   \n",
       "\n",
       "                                                    into  fox  translation  \\\n",
       "The quick brown fox jumps over the lazy dog            0    1            0   \n",
       "I love natural language processing and deep lea...     0    0            0   \n",
       "Word embeddings capture semantic relationships ...     0    0            0   \n",
       "The fox is quick and the dog is lazy                   0    1            0   \n",
       "Deep learning models learn representations auto...     0    0            0   \n",
       "\n",
       "                                                    language  to  people  \\\n",
       "The quick brown fox jumps over the lazy dog                0   0       0   \n",
       "I love natural language processing and deep lea...         1   0       0   \n",
       "Word embeddings capture semantic relationships ...         0   0       0   \n",
       "The fox is quick and the dog is lazy                       0   0       0   \n",
       "Deep learning models learn representations auto...         0   0       0   \n",
       "\n",
       "                                                    woman  kingdom  dog  room  \n",
       "The quick brown fox jumps over the lazy dog             0        0    1     0  \n",
       "I love natural language processing and deep lea...      0        0    0     0  \n",
       "Word embeddings capture semantic relationships ...      0        0    0     0  \n",
       "The fox is quick and the dog is lazy                    0        0    1     0  \n",
       "Deep learning models learn representations auto...      0        0    0     0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bow_representation = get_bow_representation(corpus, True)\n",
    "df = pd.DataFrame(bow_representation)\n",
    "df.index = corpus\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09851a0",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Documnet Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d978ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequency(corpus):\n",
    "    vocabulary = set([x for x in \" \".join(corpus).lower().split(\" \")])\n",
    "\n",
    "    term_freq = []\n",
    "    for sentence in corpus:\n",
    "        sentence_tf = dict([(v,0) for v in vocabulary])\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            sentence_tf[word] += 1\n",
    "        for v in vocabulary:\n",
    "            sentence_tf[v] /= len(word_tokenize(sentence))\n",
    "        term_freq.append(sentence_tf)\n",
    "    return term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c51b5a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.1111111111111111,\n",
       "  'and': 0.0,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.1111111111111111,\n",
       "  'lazy': 0.1111111111111111,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.0,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.1111111111111111,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.1111111111111111,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.2222222222222222,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.1111111111111111,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.1111111111111111,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.125,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.125,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.125,\n",
       "  'processing': 0.125,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.125,\n",
       "  'deep': 0.125,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.125,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.0,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.125,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.14285714285714285,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.14285714285714285,\n",
       "  'semantic': 0.14285714285714285,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.0,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.0,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.14285714285714285,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.14285714285714285,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.14285714285714285,\n",
       "  'word': 0.14285714285714285,\n",
       "  'include': 0.0,\n",
       "  'the': 0.0,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.2222222222222222,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.1111111111111111,\n",
       "  'and': 0.1111111111111111,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.1111111111111111,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.0,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.2222222222222222,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.1111111111111111,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.1111111111111111,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.0,\n",
       "  'models': 0.16666666666666666,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.16666666666666666,\n",
       "  'deep': 0.16666666666666666,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.16666666666666666,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.16666666666666666,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.16666666666666666,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.0,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.1111111111111111,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.1111111111111111,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.1111111111111111,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.1111111111111111,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.1111111111111111,\n",
       "  'about': 0.1111111111111111,\n",
       "  'learn': 0.1111111111111111,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.0,\n",
       "  'machine': 0.1111111111111111,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.1111111111111111,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.1111111111111111,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.1111111111111111,\n",
       "  'is': 0.0,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.1111111111111111,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.1111111111111111,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.0,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.1111111111111111,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.1111111111111111,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.2222222222222222,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.1111111111111111,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.1111111111111111,\n",
       "  'models': 0.0,\n",
       "  'man': 0.1111111111111111,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.1111111111111111,\n",
       "  'learning': 0.0,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.2222222222222222,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.1111111111111111,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.1111111111111111,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.1111111111111111,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.1111111111111111},\n",
       " {'them': 0.1,\n",
       "  'protects': 0.1,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.1,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.1,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.0,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.0,\n",
       "  'natural': 0.0,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.0,\n",
       "  'between': 0.0,\n",
       "  'her': 0.1,\n",
       "  'queen': 0.1,\n",
       "  'loves': 0.1,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.0,\n",
       "  'the': 0.2,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.0,\n",
       "  'language': 0.0,\n",
       "  'to': 0.0,\n",
       "  'people': 0.1,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.0},\n",
       " {'them': 0.0,\n",
       "  'protects': 0.0,\n",
       "  'fairly': 0.0,\n",
       "  'embeddings': 0.0,\n",
       "  'shared': 0.0,\n",
       "  'is': 0.0,\n",
       "  'love': 0.0,\n",
       "  'relationships': 0.0,\n",
       "  'semantic': 0.0,\n",
       "  'quick': 0.0,\n",
       "  'and': 0.14285714285714285,\n",
       "  'models': 0.0,\n",
       "  'man': 0.0,\n",
       "  'brown': 0.0,\n",
       "  'lazy': 0.0,\n",
       "  'i': 0.0,\n",
       "  'processing': 0.0,\n",
       "  'king': 0.0,\n",
       "  'walked': 0.0,\n",
       "  'learning': 0.0,\n",
       "  'deep': 0.0,\n",
       "  'capture': 0.0,\n",
       "  'jumps': 0.0,\n",
       "  'summarization': 0.14285714285714285,\n",
       "  'natural': 0.14285714285714285,\n",
       "  'automatically': 0.0,\n",
       "  'a': 0.0,\n",
       "  'nlp': 0.0,\n",
       "  'about': 0.0,\n",
       "  'learn': 0.0,\n",
       "  'tasks': 0.14285714285714285,\n",
       "  'between': 0.0,\n",
       "  'her': 0.0,\n",
       "  'queen': 0.0,\n",
       "  'loves': 0.0,\n",
       "  'representations': 0.0,\n",
       "  'over': 0.0,\n",
       "  'their': 0.0,\n",
       "  'words': 0.0,\n",
       "  'word': 0.0,\n",
       "  'include': 0.14285714285714285,\n",
       "  'the': 0.0,\n",
       "  'machine': 0.0,\n",
       "  'into': 0.0,\n",
       "  'fox': 0.0,\n",
       "  'translation': 0.14285714285714285,\n",
       "  'language': 0.14285714285714285,\n",
       "  'to': 0.0,\n",
       "  'people': 0.0,\n",
       "  'woman': 0.0,\n",
       "  'kingdom': 0.0,\n",
       "  'dog': 0.0,\n",
       "  'room': 0.0}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_term_frequency(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05cdf169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_document_frequency(corpus):\n",
    "    vocabulary = set([x for x in \" \".join(corpus).lower().split(\" \")])\n",
    "    n = len(corpus)\n",
    "\n",
    "    inverse_document_frequency = {}\n",
    "    for v in vocabulary:\n",
    "        num_docs = 0\n",
    "        for sentence in corpus:\n",
    "            if v in word_tokenize(sentence.lower()):\n",
    "                num_docs += 1\n",
    "        inverse_document_frequency[v] = np.log(n/num_docs)\n",
    "    return inverse_document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a6dbf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'them': np.float64(2.302585092994046),\n",
       " 'protects': np.float64(2.302585092994046),\n",
       " 'fairly': np.float64(2.302585092994046),\n",
       " 'embeddings': np.float64(2.302585092994046),\n",
       " 'shared': np.float64(2.302585092994046),\n",
       " 'is': np.float64(2.302585092994046),\n",
       " 'love': np.float64(1.6094379124341003),\n",
       " 'relationships': np.float64(2.302585092994046),\n",
       " 'semantic': np.float64(2.302585092994046),\n",
       " 'quick': np.float64(1.6094379124341003),\n",
       " 'and': np.float64(0.3566749439387324),\n",
       " 'models': np.float64(2.302585092994046),\n",
       " 'man': np.float64(2.302585092994046),\n",
       " 'brown': np.float64(2.302585092994046),\n",
       " 'lazy': np.float64(1.6094379124341003),\n",
       " 'i': np.float64(1.6094379124341003),\n",
       " 'processing': np.float64(2.302585092994046),\n",
       " 'king': np.float64(1.6094379124341003),\n",
       " 'walked': np.float64(2.302585092994046),\n",
       " 'learning': np.float64(1.2039728043259361),\n",
       " 'deep': np.float64(1.6094379124341003),\n",
       " 'capture': np.float64(2.302585092994046),\n",
       " 'jumps': np.float64(2.302585092994046),\n",
       " 'summarization': np.float64(2.302585092994046),\n",
       " 'natural': np.float64(1.6094379124341003),\n",
       " 'automatically': np.float64(2.302585092994046),\n",
       " 'a': np.float64(2.302585092994046),\n",
       " 'nlp': np.float64(2.302585092994046),\n",
       " 'about': np.float64(2.302585092994046),\n",
       " 'learn': np.float64(1.6094379124341003),\n",
       " 'tasks': np.float64(2.302585092994046),\n",
       " 'between': np.float64(2.302585092994046),\n",
       " 'her': np.float64(2.302585092994046),\n",
       " 'queen': np.float64(1.6094379124341003),\n",
       " 'loves': np.float64(2.302585092994046),\n",
       " 'representations': np.float64(2.302585092994046),\n",
       " 'over': np.float64(2.302585092994046),\n",
       " 'their': np.float64(2.302585092994046),\n",
       " 'words': np.float64(2.302585092994046),\n",
       " 'word': np.float64(2.302585092994046),\n",
       " 'include': np.float64(2.302585092994046),\n",
       " 'the': np.float64(0.6931471805599453),\n",
       " 'machine': np.float64(2.302585092994046),\n",
       " 'into': np.float64(2.302585092994046),\n",
       " 'fox': np.float64(1.6094379124341003),\n",
       " 'translation': np.float64(2.302585092994046),\n",
       " 'language': np.float64(1.6094379124341003),\n",
       " 'to': np.float64(2.302585092994046),\n",
       " 'people': np.float64(2.302585092994046),\n",
       " 'woman': np.float64(2.302585092994046),\n",
       " 'kingdom': np.float64(2.302585092994046),\n",
       " 'dog': np.float64(1.6094379124341003),\n",
       " 'room': np.float64(2.302585092994046)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_inverse_document_frequency(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61b8fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(corpus):\n",
    "    tf = get_term_frequency(corpus)\n",
    "    idf = get_inverse_document_frequency(corpus)\n",
    "\n",
    "    tf_idf = []\n",
    "    for tf_dict in tf:\n",
    "        tf_idf_sentence = {}\n",
    "        for t, term_freq in tf_dict.items():\n",
    "            tf_idf_sentence[t] = term_freq * idf[t]\n",
    "        tf_idf.append(tf_idf_sentence)\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da7c71ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>them</th>\n",
       "      <th>protects</th>\n",
       "      <th>fairly</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>shared</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>relationships</th>\n",
       "      <th>semantic</th>\n",
       "      <th>quick</th>\n",
       "      <th>...</th>\n",
       "      <th>into</th>\n",
       "      <th>fox</th>\n",
       "      <th>translation</th>\n",
       "      <th>language</th>\n",
       "      <th>to</th>\n",
       "      <th>people</th>\n",
       "      <th>woman</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>dog</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The quick brown fox jumps over the lazy dog</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178826</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love natural language processing and deep learning</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word embeddings capture semantic relationships between words</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.328941</td>\n",
       "      <td>0.328941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The fox is quick and the dog is lazy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511686</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178826</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep learning models learn representations automatically</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    them  protects  fairly  \\\n",
       "The quick brown fox jumps over the lazy dog          0.0       0.0     0.0   \n",
       "I love natural language processing and deep lea...   0.0       0.0     0.0   \n",
       "Word embeddings capture semantic relationships ...   0.0       0.0     0.0   \n",
       "The fox is quick and the dog is lazy                 0.0       0.0     0.0   \n",
       "Deep learning models learn representations auto...   0.0       0.0     0.0   \n",
       "\n",
       "                                                    embeddings  shared  \\\n",
       "The quick brown fox jumps over the lazy dog           0.000000     0.0   \n",
       "I love natural language processing and deep lea...    0.000000     0.0   \n",
       "Word embeddings capture semantic relationships ...    0.328941     0.0   \n",
       "The fox is quick and the dog is lazy                  0.000000     0.0   \n",
       "Deep learning models learn representations auto...    0.000000     0.0   \n",
       "\n",
       "                                                          is     love  \\\n",
       "The quick brown fox jumps over the lazy dog         0.000000  0.00000   \n",
       "I love natural language processing and deep lea...  0.000000  0.20118   \n",
       "Word embeddings capture semantic relationships ...  0.000000  0.00000   \n",
       "The fox is quick and the dog is lazy                0.511686  0.00000   \n",
       "Deep learning models learn representations auto...  0.000000  0.00000   \n",
       "\n",
       "                                                    relationships  semantic  \\\n",
       "The quick brown fox jumps over the lazy dog              0.000000  0.000000   \n",
       "I love natural language processing and deep lea...       0.000000  0.000000   \n",
       "Word embeddings capture semantic relationships ...       0.328941  0.328941   \n",
       "The fox is quick and the dog is lazy                     0.000000  0.000000   \n",
       "Deep learning models learn representations auto...       0.000000  0.000000   \n",
       "\n",
       "                                                       quick  ...  into  \\\n",
       "The quick brown fox jumps over the lazy dog         0.178826  ...   0.0   \n",
       "I love natural language processing and deep lea...  0.000000  ...   0.0   \n",
       "Word embeddings capture semantic relationships ...  0.000000  ...   0.0   \n",
       "The fox is quick and the dog is lazy                0.178826  ...   0.0   \n",
       "Deep learning models learn representations auto...  0.000000  ...   0.0   \n",
       "\n",
       "                                                         fox  translation  \\\n",
       "The quick brown fox jumps over the lazy dog         0.178826          0.0   \n",
       "I love natural language processing and deep lea...  0.000000          0.0   \n",
       "Word embeddings capture semantic relationships ...  0.000000          0.0   \n",
       "The fox is quick and the dog is lazy                0.178826          0.0   \n",
       "Deep learning models learn representations auto...  0.000000          0.0   \n",
       "\n",
       "                                                    language   to  people  \\\n",
       "The quick brown fox jumps over the lazy dog          0.00000  0.0     0.0   \n",
       "I love natural language processing and deep lea...   0.20118  0.0     0.0   \n",
       "Word embeddings capture semantic relationships ...   0.00000  0.0     0.0   \n",
       "The fox is quick and the dog is lazy                 0.00000  0.0     0.0   \n",
       "Deep learning models learn representations auto...   0.00000  0.0     0.0   \n",
       "\n",
       "                                                    woman  kingdom       dog  \\\n",
       "The quick brown fox jumps over the lazy dog           0.0      0.0  0.178826   \n",
       "I love natural language processing and deep lea...    0.0      0.0  0.000000   \n",
       "Word embeddings capture semantic relationships ...    0.0      0.0  0.000000   \n",
       "The fox is quick and the dog is lazy                  0.0      0.0  0.178826   \n",
       "Deep learning models learn representations auto...    0.0      0.0  0.000000   \n",
       "\n",
       "                                                    room  \n",
       "The quick brown fox jumps over the lazy dog          0.0  \n",
       "I love natural language processing and deep lea...   0.0  \n",
       "Word embeddings capture semantic relationships ...   0.0  \n",
       "The fox is quick and the dog is lazy                 0.0  \n",
       "Deep learning models learn representations auto...   0.0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf = get_tf_idf(corpus)\n",
    "df = pd.DataFrame(tf_idf)\n",
    "df.index = corpus\n",
    "display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
